
###############################################
Lowest value loss: 3.6400323790841496
Epoch with lowest value loss: 10

# Architecture:
Hidden layers: 1
Hidden units: [1024]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 10
Batch size: 16
Learning rate: 0.0001
Dropout rate: 0.3
Initial weight limit: 0.05
###############################################
Best:
3.6400323790841496
###############################################
Lowest value loss: 3.434218417392688
Epoch with lowest value loss: 9

# Architecture:
Hidden layers: 1
Hidden units: [1024]
Activation function: relu
Optimizer: adagrad
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 10
Batch size: 16
Learning rate: 0.01
Dropout rate: 0.3
Initial weight limit: 0.05
###############################################
Best:
3.434218417392688
###############################################
Lowest value loss: 3.2555917037519664
Epoch with lowest value loss: 10

# Architecture:
Hidden layers: 2
Hidden units: [1024, 1024]
Activation function: relu
Optimizer: adagrad
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 10
Batch size: 16
Learning rate: 0.01
Dropout rate: 0.3
Initial weight limit: 0.05
###############################################
Best:
3.2555917037519664
###############################################
Lowest value loss: 2.1292181102686443
Epoch with lowest value loss: 96

# Architecture:
Hidden layers: 2
Hidden units: [1024, 1024]
Activation function: relu
Optimizer: adagrad
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 100
Batch size: 16
Learning rate: 0.01
Dropout rate: 0.3
Initial weight limit: 0.05
###############################################
Best:
2.1292181102686443
###############################################
Lowest value loss: 1.6270107577599593
Epoch with lowest value loss: 98

# Architecture:
Hidden layers: 2
Hidden units: [1024, 1024]
Activation function: relu
Optimizer: adagrad
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 100
Batch size: 16
Learning rate: 0.01
Dropout rate: 0.3
Initial weight limit: 0.05
###############################################
Best:
1.6270107577599593
###############################################
Lowest value loss: 1.0801564993239765
Epoch with lowest value loss: 3

# Architecture:
Hidden layers: 1
Hidden units: [32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 200
Batch size: 32
Learning rate: 0.01
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
1.0801564993239765
###############################################
Lowest value loss: 0.7372470889563946
Epoch with lowest value loss: 82

# Architecture:
Hidden layers: 1
Hidden units: [32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 200
Batch size: 32
Learning rate: 0.01
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.7372470889563946
###############################################
Lowest value loss: 0.6856764056344076
Epoch with lowest value loss: 49

# Architecture:
Hidden layers: 2
Hidden units: [32, 6]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 200
Batch size: 32
Learning rate: 0.01
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.6856764056344076
###############################################
Lowest value loss: 0.5826090910339745
Epoch with lowest value loss: 96

# Architecture:
Hidden layers: 2
Hidden units: [32, 6]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 200
Batch size: 32
Learning rate: 0.01
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5826090910339745
###############################################
Lowest value loss: 0.570936995093255
Epoch with lowest value loss: 199

# Architecture:
Hidden layers: 2
Hidden units: [32, 6]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 200
Batch size: 32
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.570936995093255
###############################################
Lowest value loss: 0.459324275661903
Epoch with lowest value loss: 145

# Architecture:
Hidden layers: 2
Hidden units: [64, 6]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 400
Batch size: 32
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.459324275661903