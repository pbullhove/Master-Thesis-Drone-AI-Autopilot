
###############################################
Part: 0
Lowest value loss: 0.7558839442902858
Epoch with lowest value loss: 104

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [32, 4]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.7558839442902858
###############################################
Part: 1
Lowest value loss: 0.5580563961059251
Epoch with lowest value loss: 162

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [32, 5]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5580563961059251
###############################################
Part: 3
Lowest value loss: 0.5151039413095616
Epoch with lowest value loss: 170

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [32, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5151039413095616