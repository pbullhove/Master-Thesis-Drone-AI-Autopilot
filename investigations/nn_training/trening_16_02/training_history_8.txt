
###############################################
Part: 0
Lowest value loss: 1.029496992035395
Epoch with lowest value loss: 12

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.1
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
1.029496992035395
###############################################
Part: 1
Lowest value loss: 1.0294625048878674
Epoch with lowest value loss: 4

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.05
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
1.0294625048878674
###############################################
Part: 2
Lowest value loss: 0.9123401539207089
Epoch with lowest value loss: 30

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.01
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.9123401539207089
###############################################
Part: 3
Lowest value loss: 0.8076642863604568
Epoch with lowest value loss: 60

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.005
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.8076642863604568
###############################################
Part: 4
Lowest value loss: 0.5939236238660512
Epoch with lowest value loss: 50

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5939236238660512
###############################################
Part: 6
Lowest value loss: 0.5350664112925638
Epoch with lowest value loss: 81

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 1
Learning rate: 0.0001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5350664112925638
###############################################
Part: 13
Lowest value loss: 0.5248328103898883
Epoch with lowest value loss: 120

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 2
Learning rate: 0.0001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.5248328103898883
###############################################
Part: 18
Lowest value loss: 0.44093532626639836
Epoch with lowest value loss: 116

# Dataset:
Normalize input: True
Normalize output: True
# Architecture:
Hidden layers: 2
Hidden units: [128, 7]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 4
Learning rate: 0.001
Dropout rate: 0.2
Initial weight limit: 0.05
###############################################
Best:
0.44093532626639836