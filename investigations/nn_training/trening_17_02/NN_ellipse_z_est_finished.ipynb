{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_ellipse",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlz5c-6Sj3Dn",
        "colab_type": "text"
      },
      "source": [
        "Neural Network for detecting ellipses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF9YnVXDg8Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from keras.optimizers import Adam, SGD, Adagrad\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.initializers import RandomUniform\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras import losses\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkRFZEJNd2Ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_train = '/content/drive/My Drive/Colab_data/full_dataset_train.json'\n",
        "with open(filepath_train) as json_file:\n",
        "    training_data = json.load(json_file)\n",
        "\n",
        "filepath_test = '/content/drive/My Drive/Colab_data/full_dataset_test.json'\n",
        "with open(filepath_test) as json_file:\n",
        "    test_data = json.load(json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQkeZnkAg-8N",
        "colab_type": "code",
        "outputId": "49e13944-7230-4922-f2fa-6ac654c3e2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(training_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4891\n",
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOlgVOqHhVZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_and_shuffle_data(data):\n",
        "    # min_height = 1.2\n",
        "\n",
        "    len_data = len(data)\n",
        "    lst = []\n",
        "\n",
        "    for img_idx in range(len_data):\n",
        "        data_point = data[str(img_idx)]\n",
        "        A = data_point['ellipse'][0]\n",
        "        B = data_point['ellipse'][1]\n",
        "        C = data_point['ellipse'][2]\n",
        "        D = data_point['ellipse'][3]\n",
        "        E = data_point['ellipse'][4]\n",
        "        F = data_point['ellipse'][5]\n",
        "\n",
        "        inner_square = math.sqrt( (A-C)**2 + B**2)\n",
        "        outside = 1.0 / (B**2 - 4*A*C)\n",
        "        a = outside * math.sqrt(2*(A*E**2 + C*D**2 - B*D*E + (B**2 - 4*A*C)*F) * ( (A+C) + inner_square))\n",
        "        b = outside * math.sqrt(2*(A*E**2 + C*D**2 - B*D*E + (B**2 - 4*A*C)*F) * ( (A+C) - inner_square))\n",
        "\n",
        "        ellipse = np.array([a,b])\n",
        "\n",
        "        # ellipse = np.array(data_point['ellipse'])\n",
        "        gt = np.array(data_point['ground_truth'])\n",
        "        # if gt[2] > min_height:\n",
        "        lst.append([ellipse, gt])\n",
        "\n",
        "    array = np.array(lst)\n",
        "    np.random.shuffle(array)\n",
        "    x, y = np.transpose(array, axes=(1,0))\n",
        "    return np.asarray(x.tolist()), np.asarray(y.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZZcNupmDC0",
        "colab_type": "code",
        "outputId": "d22e61b0-a6c2-43d4-88c1-91d6e8f69072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x_train, y_train = split_and_shuffle_data(training_data)\n",
        "x_test, y_test = split_and_shuffle_data(test_data)\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_test))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4891\n",
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NjDeEzGG-ih",
        "colab_type": "code",
        "outputId": "3e9ad76d-9217-4b08-d555-13fc1a6a7ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check data\n",
        "# b**2 - 4*a*c < 0\n",
        "count = 0\n",
        "for parameters in x_train:\n",
        "    if parameters[1]**2 - 4*parameters[0]*parameters[2] == 0:\n",
        "        count += 1\n",
        "print(count)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsWXAWkSk5Hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_and_save_training_history(history, run_id, part_id=0):\n",
        "    # Plot training & validation accuracy values\n",
        "    # plt.plot(history.history['acc'])\n",
        "    # plt.plot(history.history['val_acc'])\n",
        "    # plt.title('Model accuracy')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    # plt.savefig('model_'+str(run_id)+'accuracy.svg')\n",
        "    # plt.savefig('model_'+str(run_id)+'accuracy.png')\n",
        "    # plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylim(top=2.0)\n",
        "    plt.ylim(bottom=0.0)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    # plt.savefig('images/model_'+str(run_id)+'_'+str(part_id)+'loss.svg')\n",
        "    # plt.savefig('images/model_'+str(run_id)+'_'+str(part_id)+'loss.png')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjSgccGE0YUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_and_save_multi_training_history(history, run_id, part_id=0):\n",
        "    # Plot training & validation loss values\n",
        "    plt.figure(figsize=(6, 8))\n",
        "    plt.subplot(3, 1, 1)\n",
        "\n",
        "    plt.plot(history.history['output_x_loss'])\n",
        "    plt.plot(history.history['val_output_x_loss'])\n",
        "    plt.title('Model loss x')\n",
        "    plt.ylim(top=1.0)\n",
        "    plt.ylim(bottom=0.0)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    \n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(history.history['output_y_loss'])\n",
        "    plt.plot(history.history['val_output_y_loss'])\n",
        "    plt.title('Model loss y')\n",
        "    plt.ylim(top=1.0)\n",
        "    plt.ylim(bottom=0.0)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    \n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(history.history['output_z_loss'])\n",
        "    plt.plot(history.history['val_output_z_loss'])\n",
        "    plt.title('Model loss z')\n",
        "    plt.ylim(top=1.0)\n",
        "    plt.ylim(bottom=0.0)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    \n",
        "    \n",
        "    plt.savefig('images/model_'+str(run_id)+'_'+str(part_id)+'multi_loss.svg')\n",
        "    plt.savefig('images/model_'+str(run_id)+'_'+str(part_id)+'multi_loss.png')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD7NoiIyBvdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_report(run_id, min_val_loss, min_val_loss_id, init_weight_limit,\n",
        "                hidden_units, activation_fn, dropout_rate, batch_size,\n",
        "                max_epochs, learning_rate, l2_reg, use_optimizer, use_loss,\n",
        "                normalize_input, normalize_output, part_id=0):\n",
        "    filename = 'history/training_history_'+str(run_id)+'.txt'\n",
        "    \n",
        "    print(\"Best loss:\", min_val_loss)\n",
        "    print(\"Best loss id:\", min_val_loss_id)\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.read().splitlines()\n",
        "            last_line = lines[-1]\n",
        "            prev_best_val_loss = float(last_line)\n",
        "            print(\"Previous best loss:\", prev_best_val_loss)\n",
        "        \n",
        "        # Only write to file if better than previous\n",
        "        if min_val_loss > prev_best_val_loss:\n",
        "            print(\"Not better than previous run\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"New best!\")\n",
        "    except:\n",
        "        print(\"This is the first entry in the file\")\n",
        "\n",
        "    # Open a file with access mode 'a'\n",
        "    file_object = open(filename, 'a')\n",
        "\n",
        "    file_object.write('\\n###############################################\\n')\n",
        "    file_object.write('Part: '+str(part_id)+'\\n')\n",
        "    file_object.write('Lowest value loss: ' + str(min_val_loss) + '\\n')\n",
        "    file_object.write('Epoch with lowest value loss: ' + str(min_val_loss_id) + '\\n')\n",
        "    file_object.write('\\n')\n",
        "\n",
        "    file_object.write('# Dataset:\\n')\n",
        "    file_object.write('Normalize input: '+str(normalize_input)+'\\n')\n",
        "    file_object.write('Normalize output: '+str(normalize_output)+'\\n')\n",
        "    file_object.write('\\n')\n",
        "\n",
        "    file_object.write('# Architecture:\\n')\n",
        "    file_object.write('Hidden layers: '+str(len(hidden_units))+'\\n')\n",
        "    file_object.write('Hidden units: '+str(hidden_units)+'\\n')\n",
        "    file_object.write('Activation function: '+str(activation_fn)+'\\n')\n",
        "    file_object.write('Optimizer: '+str(use_optimizer)+'\\n')\n",
        "    file_object.write('Loss function: '+str(use_loss)+'\\n')\n",
        "    file_object.write('\\n')\n",
        "\n",
        "    file_object.write('# Hyper parameters:\\n')\n",
        "    file_object.write('Max epochs: '+str(max_epochs)+'\\n')\n",
        "    file_object.write('Batch size: '+str(batch_size)+'\\n')\n",
        "    file_object.write('Learning rate: '+str(learning_rate)+'\\n')\n",
        "    file_object.write('L2 regularization: '+str(l2_reg)+'\\n')\n",
        "    file_object.write('Dropout rate: '+str(dropout_rate)+'\\n')\n",
        "    file_object.write('Initial weight limit: '+str(init_weight_limit)+'\\n')\n",
        "    file_object.write('###############################################\\n')\n",
        "    file_object.write('Best:\\n')\n",
        "    file_object.write(str(min_val_loss))\n",
        "\n",
        "    # Close the file\n",
        "    file_object.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCYBi6AphXrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(norm_input=False, norm_output=False):\n",
        "\n",
        "    # Choose the first 4 input layers\n",
        "\n",
        "    # Normalize the data\n",
        "    x_mean = np.mean(x_train, axis = 0)\n",
        "    x_std = np.std(x_train, axis = 0)\n",
        "\n",
        "    y_mean = np.mean(y_train, axis = 0)\n",
        "    y_std = np.std(y_train, axis = 0)\n",
        "\n",
        "    x_train_std = (x_train - x_mean) / x_std\n",
        "    y_train_std = (y_train - y_mean) / y_std\n",
        "\n",
        "    x_test_std = (x_test - x_mean) / x_std\n",
        "    y_test_std = (y_test - y_mean) / y_std\n",
        "\n",
        "    ########################################\n",
        "    # Choose between\n",
        "    # standarized or not standarized\n",
        "    # input and output data\n",
        "    if norm_input:\n",
        "        x_train_in_use_l = x_train_std\n",
        "        x_test_in_use_l = x_test_std\n",
        "    else:\n",
        "        x_train_in_use_l = x_train\n",
        "        x_test_in_use_l = x_test\n",
        "\n",
        "    if norm_output:\n",
        "        y_train_in_use_l = y_train_std\n",
        "        y_test_in_use_l = y_test_std\n",
        "    else:\n",
        "        y_train_in_use_l = y_train\n",
        "        y_test_in_use_l = y_test\n",
        "    \n",
        "    return x_train_in_use_l, x_test_in_use_l, y_train_in_use_l, y_test_in_use_l, y_mean, y_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnzrk__QF7jH",
        "colab_type": "code",
        "outputId": "f62f78dd-9bef-4ded-ea53-9f2953eced10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_model(prev_best_loss, run_id, part_id, init_weight_limit, hidden_units, activation_fn,\n",
        "                    dropout_rate, batch_size, max_epochs, learning_rate, l2_reg,\n",
        "                    use_optimizer, use_loss, normalize_input, normalize_output):\n",
        "    # Set up the dataset\n",
        "    (x_train_in_use, x_test_in_use,\n",
        "    y_train_in_use, y_test_in_use, y_mean, y_std) = normalize_data(norm_input=normalize_input,\n",
        "                                                        norm_output=normalize_output)\n",
        "    \n",
        "    num_inputs = 2\n",
        "\n",
        "    # x_train_in_use = x_train_in_use[:,[0,2,3,4,5]]\n",
        "    # x_test_in_use = x_test_in_use[:,[0,2,3,4,5]]\n",
        "\n",
        "    y_train_x = y_train_in_use[:,0]\n",
        "    y_train_y = y_train_in_use[:,1]\n",
        "    y_train_z = y_train_in_use[:,2]\n",
        "\n",
        "    y_test_x = y_test_in_use[:,0]\n",
        "    y_test_y = y_test_in_use[:,1]\n",
        "    y_test_z = y_test_in_use[:,2]\n",
        "    ####################################\n",
        "\n",
        "    random_uniform_initializer = RandomUniform(minval=-init_weight_limit, maxval=init_weight_limit, seed=None)\n",
        "\n",
        "    # Create model\n",
        "    inputs = Input(shape=(num_inputs,), name=\"input\")\n",
        "\n",
        "    # layer_x = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "    #                 kernel_initializer=random_uniform_initializer, name=\"hidden_x\")(inputs)\n",
        "    # layer_y = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "    #                 kernel_initializer=random_uniform_initializer, name=\"hidden_y\")(inputs)\n",
        "    layer_z = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "                    kernel_initializer=random_uniform_initializer,\n",
        "                    kernel_regularizer=regularizers.l2(l2_reg),                    \n",
        "                    name=\"hidden_z\")(inputs)\n",
        "\n",
        "    # Add more layers if hidden_layers > 1\n",
        "    for i in range(len(hidden_units)-1):\n",
        "        # layer_x = Dense(units=hidden_units[i], activation=activation_fn,\n",
        "        #             kernel_initializer=random_uniform_initializer, name=\"hidden_x_\"+str(i))(layer_x)\n",
        "        # layer_y = Dense(units=hidden_units[i], activation=activation_fn,\n",
        "        #                 kernel_initializer=random_uniform_initializer, name=\"hidden_y_\"+str(i))(layer_y)\n",
        "        layer_z = Dense(units=hidden_units[i+1], activation=activation_fn,\n",
        "                        kernel_initializer=random_uniform_initializer,\n",
        "                        kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                        name=\"hidden_z_\"+str(i))(layer_z)\n",
        "\n",
        "    # output_x = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_x\")(layer_x)\n",
        "    # output_y = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_y\")(layer_y)\n",
        "    output_z = Dense(units=1, kernel_initializer=random_uniform_initializer,\n",
        "                     kernel_regularizer=regularizers.l2(l2_reg),\n",
        "                     name=\"output_z\")(layer_z)\n",
        "\n",
        "    # model = Model(inputs=inputs, outputs=[output_x, output_y, output_z])\n",
        "    model = None\n",
        "    model = Model(inputs=inputs, outputs=output_z)\n",
        "\n",
        "    # Choose optimizer\n",
        "    if use_optimizer == 'adam':\n",
        "        # Original : adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "        opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "    elif use_optimizer == 'sgd':\n",
        "        # Original: sgd = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
        "        opt = SGD(lr=learning_rate)\n",
        "    elif use_optimizer == 'adagrad':\n",
        "        # Original: adagrad = Adagrad(learning_rate=0.01) Recommended to use lr=0.01\n",
        "        opt = Adagrad(lr=learning_rate)\n",
        "    else:\n",
        "        opt = None\n",
        "\n",
        "    # model.compile(loss={'output_x': use_loss, 'output_y': use_loss, 'output_z': 'mae'}, optimizer=opt)\n",
        "    model.compile(loss=use_loss, optimizer=opt)\n",
        "\n",
        "    # Run this to train the model\n",
        "    STAMP = 'models/best_model_run_'+str(run_id)+'_'+str(part_id)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "    bst_model_path = STAMP + '.h5'\n",
        "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    # history = model.fit(x_train_std, [y_train_x, y_train_y, y_train_z],\n",
        "    history = model.fit(x_train_in_use, y_train_z,\n",
        "                        validation_split=0.2,\n",
        "                        # validation_data=({'input':x_test_in_use}, {'output_x':y_test_x, 'output_y':y_test_y, 'output_z':y_test_z}),\n",
        "                        epochs=max_epochs, batch_size=batch_size, verbose=1,\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "    val_loss_history = history.history['val_loss']\n",
        "    min_val_loss = min(val_loss_history)\n",
        "    min_val_loss_id = val_loss_history.index(min_val_loss) + 1 \n",
        "\n",
        "    make_report(run_id, min_val_loss, min_val_loss_id, init_weight_limit,\n",
        "        hidden_units, activation_fn, dropout_rate, batch_size,\n",
        "        max_epochs, learning_rate, l2_reg, use_optimizer, use_loss,\n",
        "        normalize_input, normalize_output, part_id)\n",
        "\n",
        "    # show_and_save_multi_training_history(history, run_id, part_id)\n",
        "    show_and_save_training_history(history, run_id, part_id)\n",
        "    # test_model()\n",
        "\n",
        "    # Save only the best\n",
        "    if min_val_loss < prev_best_loss:\n",
        "        model.save('models/nn_model_'+str(run_id)+'_'+str(part_id)+'.h5')\n",
        "        return min_val_loss\n",
        "    else:\n",
        "        return prev_best_loss\n",
        "\n",
        "####################################\n",
        "# Hyperparameters and architecture\n",
        "run_id = 6\n",
        "\n",
        "init_weight_limit = 0.1\n",
        "hidden_units = [256, 6, 32]\n",
        "activation_fn = 'relu'\n",
        "dropout_rate = 0.2\n",
        "batch_size = 32\n",
        "max_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "l2_reg = 0.0000001\n",
        "use_optimizer = 'adam' # {'adam', 'sgd'}\n",
        "use_loss = 'mean_squared_error' # {'mean_squared_error' , 'mean_squared_logarithmic_error', 'mean_absolute_error'}\n",
        "normalize_input = True\n",
        "normalize_output = True\n",
        "####################################\n",
        "\n",
        "# test_list = []\n",
        "test_list = [\n",
        "    [256, 12, 32],\n",
        "    [256, 24, 32],\n",
        "    [256, 32, 32],\n",
        "]\n",
        "\n",
        "# for i in units_list:\n",
        "#     for j in units_list:\n",
        "#         test_list.append([i, j])\n",
        "\n",
        "prev_best_loss = 1000\n",
        "part_id = 0\n",
        "# for part_id in range(len(test_list)):\n",
        "#     hidden_units = test_list[part_id]\n",
        "#     print('hidden_units: ', end=\"\")\n",
        "#     print(hidden_units)\n",
        "\n",
        "prev_best_loss = train_model(prev_best_loss, run_id, part_id, init_weight_limit, hidden_units, activation_fn,\n",
        "        dropout_rate, batch_size, max_epochs, learning_rate, l2_reg,\n",
        "        use_optimizer, use_loss, normalize_input, normalize_output)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3912 samples, validate on 979 samples\n",
            "Epoch 1/1000\n",
            "3912/3912 [==============================] - 2s 623us/step - loss: 0.5812 - val_loss: 0.3012\n",
            "Epoch 2/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 0.1696 - val_loss: 0.0870\n",
            "Epoch 3/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 0.0423 - val_loss: 0.0158\n",
            "Epoch 4/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 0.0068 - val_loss: 0.0038\n",
            "Epoch 5/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 0.0020 - val_loss: 0.0018\n",
            "Epoch 6/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 7/1000\n",
            "3912/3912 [==============================] - 1s 152us/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 8/1000\n",
            "3912/3912 [==============================] - 1s 166us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 9/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 9.4634e-04 - val_loss: 0.0011\n",
            "Epoch 10/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 9.4601e-04 - val_loss: 0.0020\n",
            "Epoch 11/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 8.8608e-04 - val_loss: 9.9266e-04\n",
            "Epoch 12/1000\n",
            "3912/3912 [==============================] - 0s 123us/step - loss: 9.4172e-04 - val_loss: 0.0010\n",
            "Epoch 13/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 8.5722e-04 - val_loss: 9.3203e-04\n",
            "Epoch 14/1000\n",
            "3912/3912 [==============================] - 0s 121us/step - loss: 8.9750e-04 - val_loss: 9.5431e-04\n",
            "Epoch 15/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 8.3698e-04 - val_loss: 0.0011\n",
            "Epoch 16/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 8.8416e-04 - val_loss: 0.0011\n",
            "Epoch 17/1000\n",
            "3912/3912 [==============================] - 1s 150us/step - loss: 8.0379e-04 - val_loss: 8.7948e-04\n",
            "Epoch 18/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 9.3372e-04 - val_loss: 9.3459e-04\n",
            "Epoch 19/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 8.6561e-04 - val_loss: 0.0011\n",
            "Epoch 20/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 7.7591e-04 - val_loss: 8.8339e-04\n",
            "Epoch 21/1000\n",
            "3912/3912 [==============================] - 1s 145us/step - loss: 7.3703e-04 - val_loss: 8.9784e-04\n",
            "Epoch 22/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 7.2443e-04 - val_loss: 8.3936e-04\n",
            "Epoch 23/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 7.3621e-04 - val_loss: 8.5171e-04\n",
            "Epoch 24/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 7.7253e-04 - val_loss: 8.3825e-04\n",
            "Epoch 25/1000\n",
            "3912/3912 [==============================] - 1s 139us/step - loss: 7.0923e-04 - val_loss: 8.5863e-04\n",
            "Epoch 26/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 7.7911e-04 - val_loss: 0.0012\n",
            "Epoch 27/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 8.6880e-04 - val_loss: 0.0012\n",
            "Epoch 28/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 8.4663e-04 - val_loss: 8.0372e-04\n",
            "Epoch 29/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 7.3961e-04 - val_loss: 8.6868e-04\n",
            "Epoch 30/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 8.3255e-04 - val_loss: 0.0010\n",
            "Epoch 31/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 7.6323e-04 - val_loss: 8.2355e-04\n",
            "Epoch 32/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 8.2831e-04 - val_loss: 8.3713e-04\n",
            "Epoch 33/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 7.5618e-04 - val_loss: 0.0012\n",
            "Epoch 34/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 7.5580e-04 - val_loss: 9.0433e-04\n",
            "Epoch 35/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.9953e-04 - val_loss: 0.0010\n",
            "Epoch 36/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 6.9683e-04 - val_loss: 8.3582e-04\n",
            "Epoch 37/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 7.8847e-04 - val_loss: 8.0782e-04\n",
            "Epoch 38/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 7.6641e-04 - val_loss: 9.9694e-04\n",
            "Epoch 39/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 7.8255e-04 - val_loss: 9.6469e-04\n",
            "Epoch 40/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 6.7506e-04 - val_loss: 7.4386e-04\n",
            "Epoch 41/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 7.9788e-04 - val_loss: 0.0010\n",
            "Epoch 42/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 7.7397e-04 - val_loss: 8.4083e-04\n",
            "Epoch 43/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 6.6882e-04 - val_loss: 8.1419e-04\n",
            "Epoch 44/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 7.0120e-04 - val_loss: 7.2697e-04\n",
            "Epoch 45/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 7.1730e-04 - val_loss: 8.8995e-04\n",
            "Epoch 46/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 7.3349e-04 - val_loss: 8.3185e-04\n",
            "Epoch 47/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 8.4822e-04 - val_loss: 9.3174e-04\n",
            "Epoch 48/1000\n",
            "3912/3912 [==============================] - 0s 121us/step - loss: 8.7110e-04 - val_loss: 0.0010\n",
            "Epoch 49/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.9395e-04 - val_loss: 8.5447e-04\n",
            "Epoch 50/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 7.3013e-04 - val_loss: 7.3580e-04\n",
            "Epoch 51/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.6117e-04 - val_loss: 9.9122e-04\n",
            "Epoch 52/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 7.0835e-04 - val_loss: 7.9951e-04\n",
            "Epoch 53/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 7.0552e-04 - val_loss: 0.0017\n",
            "Epoch 54/1000\n",
            "3912/3912 [==============================] - 1s 141us/step - loss: 9.2209e-04 - val_loss: 0.0016\n",
            "Epoch 55/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 7.3538e-04 - val_loss: 6.9931e-04\n",
            "Epoch 56/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.7969e-04 - val_loss: 0.0010\n",
            "Epoch 57/1000\n",
            "3912/3912 [==============================] - 0s 119us/step - loss: 6.6432e-04 - val_loss: 7.1077e-04\n",
            "Epoch 58/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.9122e-04 - val_loss: 8.9971e-04\n",
            "Epoch 59/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 7.8604e-04 - val_loss: 7.8044e-04\n",
            "Epoch 60/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 7.1370e-04 - val_loss: 7.8406e-04\n",
            "Epoch 61/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.9600e-04 - val_loss: 9.3536e-04\n",
            "Epoch 62/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.8206e-04 - val_loss: 7.5889e-04\n",
            "Epoch 63/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.4723e-04 - val_loss: 7.6078e-04\n",
            "Epoch 64/1000\n",
            "3912/3912 [==============================] - 1s 143us/step - loss: 7.4250e-04 - val_loss: 7.5042e-04\n",
            "Epoch 65/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.9961e-04 - val_loss: 8.5114e-04\n",
            "Epoch 66/1000\n",
            "3912/3912 [==============================] - 1s 141us/step - loss: 7.0110e-04 - val_loss: 7.0124e-04\n",
            "Epoch 67/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.5913e-04 - val_loss: 9.2643e-04\n",
            "Epoch 68/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 8.6902e-04 - val_loss: 0.0010\n",
            "Epoch 69/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 7.2080e-04 - val_loss: 0.0011\n",
            "Epoch 70/1000\n",
            "3912/3912 [==============================] - 1s 167us/step - loss: 6.8254e-04 - val_loss: 6.7232e-04\n",
            "Epoch 71/1000\n",
            "3912/3912 [==============================] - 1s 152us/step - loss: 7.0393e-04 - val_loss: 7.3902e-04\n",
            "Epoch 72/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.7480e-04 - val_loss: 0.0012\n",
            "Epoch 73/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 7.1105e-04 - val_loss: 6.8127e-04\n",
            "Epoch 74/1000\n",
            "3912/3912 [==============================] - 1s 144us/step - loss: 6.5916e-04 - val_loss: 7.0637e-04\n",
            "Epoch 75/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.3223e-04 - val_loss: 7.1902e-04\n",
            "Epoch 76/1000\n",
            "3912/3912 [==============================] - 0s 123us/step - loss: 7.3817e-04 - val_loss: 8.2975e-04\n",
            "Epoch 77/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 7.8110e-04 - val_loss: 9.0831e-04\n",
            "Epoch 78/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 6.5423e-04 - val_loss: 8.2997e-04\n",
            "Epoch 79/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 6.7501e-04 - val_loss: 9.5150e-04\n",
            "Epoch 80/1000\n",
            "3912/3912 [==============================] - 1s 167us/step - loss: 6.6463e-04 - val_loss: 6.6008e-04\n",
            "Epoch 81/1000\n",
            "3912/3912 [==============================] - 1s 151us/step - loss: 7.3759e-04 - val_loss: 7.0322e-04\n",
            "Epoch 82/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.7867e-04 - val_loss: 0.0011\n",
            "Epoch 83/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 7.2243e-04 - val_loss: 6.9517e-04\n",
            "Epoch 84/1000\n",
            "3912/3912 [==============================] - 1s 170us/step - loss: 6.6792e-04 - val_loss: 9.6784e-04\n",
            "Epoch 85/1000\n",
            "3912/3912 [==============================] - 1s 153us/step - loss: 7.0841e-04 - val_loss: 8.0330e-04\n",
            "Epoch 86/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 7.9522e-04 - val_loss: 8.3518e-04\n",
            "Epoch 87/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.8740e-04 - val_loss: 0.0012\n",
            "Epoch 88/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.9534e-04 - val_loss: 0.0010\n",
            "Epoch 89/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 7.1048e-04 - val_loss: 0.0011\n",
            "Epoch 90/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 6.8417e-04 - val_loss: 7.3853e-04\n",
            "Epoch 91/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.3999e-04 - val_loss: 6.7130e-04\n",
            "Epoch 92/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.6959e-04 - val_loss: 7.1902e-04\n",
            "Epoch 93/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.4962e-04 - val_loss: 6.9789e-04\n",
            "Epoch 94/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.6970e-04 - val_loss: 8.3805e-04\n",
            "Epoch 95/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.5880e-04 - val_loss: 7.1057e-04\n",
            "Epoch 96/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.7354e-04 - val_loss: 7.5701e-04\n",
            "Epoch 97/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 6.7795e-04 - val_loss: 8.5517e-04\n",
            "Epoch 98/1000\n",
            "3912/3912 [==============================] - 0s 128us/step - loss: 7.4658e-04 - val_loss: 7.2947e-04\n",
            "Epoch 99/1000\n",
            "3912/3912 [==============================] - 0s 128us/step - loss: 6.5661e-04 - val_loss: 7.8271e-04\n",
            "Epoch 100/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.8599e-04 - val_loss: 7.4240e-04\n",
            "Epoch 101/1000\n",
            "3912/3912 [==============================] - 0s 123us/step - loss: 7.2302e-04 - val_loss: 0.0010\n",
            "Epoch 102/1000\n",
            "3912/3912 [==============================] - 0s 123us/step - loss: 6.3353e-04 - val_loss: 6.3752e-04\n",
            "Epoch 103/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 8.0305e-04 - val_loss: 7.4297e-04\n",
            "Epoch 104/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.9113e-04 - val_loss: 6.5416e-04\n",
            "Epoch 105/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 7.0242e-04 - val_loss: 6.7359e-04\n",
            "Epoch 106/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 5.9670e-04 - val_loss: 6.5910e-04\n",
            "Epoch 107/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 6.1197e-04 - val_loss: 6.6863e-04\n",
            "Epoch 108/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.2768e-04 - val_loss: 6.7358e-04\n",
            "Epoch 109/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.8314e-04 - val_loss: 6.6610e-04\n",
            "Epoch 110/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 5.9034e-04 - val_loss: 0.0011\n",
            "Epoch 111/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 6.2730e-04 - val_loss: 7.1279e-04\n",
            "Epoch 112/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 6.2347e-04 - val_loss: 6.7888e-04\n",
            "Epoch 113/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.5335e-04 - val_loss: 9.3850e-04\n",
            "Epoch 114/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 7.4477e-04 - val_loss: 6.5999e-04\n",
            "Epoch 115/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 7.4045e-04 - val_loss: 7.3306e-04\n",
            "Epoch 116/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 7.6012e-04 - val_loss: 9.8126e-04\n",
            "Epoch 117/1000\n",
            "3912/3912 [==============================] - 0s 123us/step - loss: 6.6869e-04 - val_loss: 6.8701e-04\n",
            "Epoch 118/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.3630e-04 - val_loss: 7.1992e-04\n",
            "Epoch 119/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.9625e-04 - val_loss: 0.0012\n",
            "Epoch 120/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.6480e-04 - val_loss: 0.0011\n",
            "Epoch 121/1000\n",
            "3912/3912 [==============================] - 1s 157us/step - loss: 6.1185e-04 - val_loss: 6.3549e-04\n",
            "Epoch 122/1000\n",
            "3912/3912 [==============================] - 1s 166us/step - loss: 6.6899e-04 - val_loss: 8.9736e-04\n",
            "Epoch 123/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.3644e-04 - val_loss: 9.0548e-04\n",
            "Epoch 124/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 7.1685e-04 - val_loss: 6.6401e-04\n",
            "Epoch 125/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.3734e-04 - val_loss: 6.8911e-04\n",
            "Epoch 126/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 5.9538e-04 - val_loss: 7.5486e-04\n",
            "Epoch 127/1000\n",
            "3912/3912 [==============================] - 1s 143us/step - loss: 6.9006e-04 - val_loss: 6.6522e-04\n",
            "Epoch 128/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 6.0518e-04 - val_loss: 7.2182e-04\n",
            "Epoch 129/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.6601e-04 - val_loss: 8.8669e-04\n",
            "Epoch 130/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.5081e-04 - val_loss: 7.5033e-04\n",
            "Epoch 131/1000\n",
            "3912/3912 [==============================] - 1s 148us/step - loss: 6.4207e-04 - val_loss: 6.3309e-04\n",
            "Epoch 132/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.5623e-04 - val_loss: 0.0015\n",
            "Epoch 133/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.7221e-04 - val_loss: 6.6435e-04\n",
            "Epoch 134/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 6.7165e-04 - val_loss: 7.6891e-04\n",
            "Epoch 135/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 6.4259e-04 - val_loss: 7.2710e-04\n",
            "Epoch 136/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 7.9817e-04 - val_loss: 6.3411e-04\n",
            "Epoch 137/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 7.0639e-04 - val_loss: 6.4261e-04\n",
            "Epoch 138/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 6.0116e-04 - val_loss: 8.5086e-04\n",
            "Epoch 139/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 6.2646e-04 - val_loss: 0.0012\n",
            "Epoch 140/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 6.5728e-04 - val_loss: 0.0011\n",
            "Epoch 141/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 6.6044e-04 - val_loss: 6.8077e-04\n",
            "Epoch 142/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.3922e-04 - val_loss: 6.7412e-04\n",
            "Epoch 143/1000\n",
            "3912/3912 [==============================] - 1s 141us/step - loss: 6.2852e-04 - val_loss: 6.4989e-04\n",
            "Epoch 144/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.4246e-04 - val_loss: 8.4273e-04\n",
            "Epoch 145/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.7901e-04 - val_loss: 7.2479e-04\n",
            "Epoch 146/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.0045e-04 - val_loss: 8.5878e-04\n",
            "Epoch 147/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 5.9898e-04 - val_loss: 6.3841e-04\n",
            "Epoch 148/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.0400e-04 - val_loss: 7.8750e-04\n",
            "Epoch 149/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 6.7797e-04 - val_loss: 6.9529e-04\n",
            "Epoch 150/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 8.2249e-04 - val_loss: 7.7870e-04\n",
            "Epoch 151/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 8.2241e-04 - val_loss: 7.9412e-04\n",
            "Epoch 152/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 6.2188e-04 - val_loss: 6.4722e-04\n",
            "Epoch 153/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 5.9959e-04 - val_loss: 0.0011\n",
            "Epoch 154/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 6.8206e-04 - val_loss: 0.0021\n",
            "Epoch 155/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.7470e-04 - val_loss: 6.3584e-04\n",
            "Epoch 156/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.1489e-04 - val_loss: 7.2407e-04\n",
            "Epoch 157/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 6.3459e-04 - val_loss: 8.5922e-04\n",
            "Epoch 158/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 6.6146e-04 - val_loss: 6.2669e-04\n",
            "Epoch 159/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 7.0943e-04 - val_loss: 7.9425e-04\n",
            "Epoch 160/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 6.2210e-04 - val_loss: 6.7402e-04\n",
            "Epoch 161/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 7.1694e-04 - val_loss: 6.4415e-04\n",
            "Epoch 162/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.1988e-04 - val_loss: 7.7263e-04\n",
            "Epoch 163/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.7551e-04 - val_loss: 0.0015\n",
            "Epoch 164/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.8771e-04 - val_loss: 8.8656e-04\n",
            "Epoch 165/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 6.5088e-04 - val_loss: 6.3749e-04\n",
            "Epoch 166/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.3448e-04 - val_loss: 6.5828e-04\n",
            "Epoch 167/1000\n",
            "3912/3912 [==============================] - 1s 142us/step - loss: 6.6523e-04 - val_loss: 0.0010\n",
            "Epoch 168/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 7.0741e-04 - val_loss: 7.6075e-04\n",
            "Epoch 169/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 7.2426e-04 - val_loss: 6.8077e-04\n",
            "Epoch 170/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.7611e-04 - val_loss: 7.9182e-04\n",
            "Epoch 171/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 6.6861e-04 - val_loss: 7.4642e-04\n",
            "Epoch 172/1000\n",
            "3912/3912 [==============================] - 1s 136us/step - loss: 6.5152e-04 - val_loss: 9.7530e-04\n",
            "Epoch 173/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.3451e-04 - val_loss: 6.6147e-04\n",
            "Epoch 174/1000\n",
            "3912/3912 [==============================] - 1s 131us/step - loss: 7.1602e-04 - val_loss: 6.6500e-04\n",
            "Epoch 175/1000\n",
            "3912/3912 [==============================] - 1s 128us/step - loss: 6.0591e-04 - val_loss: 7.7449e-04\n",
            "Epoch 176/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.2503e-04 - val_loss: 7.4240e-04\n",
            "Epoch 177/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.6084e-04 - val_loss: 8.2576e-04\n",
            "Epoch 178/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.7167e-04 - val_loss: 6.8706e-04\n",
            "Epoch 179/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 7.0554e-04 - val_loss: 7.0548e-04\n",
            "Epoch 180/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 5.8735e-04 - val_loss: 6.2803e-04\n",
            "Epoch 181/1000\n",
            "3912/3912 [==============================] - 1s 137us/step - loss: 6.8270e-04 - val_loss: 6.3100e-04\n",
            "Epoch 182/1000\n",
            "3912/3912 [==============================] - 1s 135us/step - loss: 5.9909e-04 - val_loss: 7.2850e-04\n",
            "Epoch 183/1000\n",
            "3912/3912 [==============================] - 1s 170us/step - loss: 6.6571e-04 - val_loss: 7.9539e-04\n",
            "Epoch 184/1000\n",
            "3912/3912 [==============================] - 1s 162us/step - loss: 6.3727e-04 - val_loss: 6.4542e-04\n",
            "Epoch 185/1000\n",
            "3912/3912 [==============================] - 1s 139us/step - loss: 5.8621e-04 - val_loss: 0.0011\n",
            "Epoch 186/1000\n",
            "3912/3912 [==============================] - 1s 144us/step - loss: 6.3728e-04 - val_loss: 6.2040e-04\n",
            "Epoch 187/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 7.0455e-04 - val_loss: 6.6636e-04\n",
            "Epoch 188/1000\n",
            "3912/3912 [==============================] - 1s 147us/step - loss: 6.1140e-04 - val_loss: 6.2209e-04\n",
            "Epoch 189/1000\n",
            "3912/3912 [==============================] - 0s 122us/step - loss: 6.9946e-04 - val_loss: 6.7701e-04\n",
            "Epoch 190/1000\n",
            "3912/3912 [==============================] - 0s 125us/step - loss: 6.7925e-04 - val_loss: 6.2535e-04\n",
            "Epoch 191/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 5.7264e-04 - val_loss: 6.4504e-04\n",
            "Epoch 192/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 6.8124e-04 - val_loss: 8.1417e-04\n",
            "Epoch 193/1000\n",
            "3912/3912 [==============================] - 1s 159us/step - loss: 6.6382e-04 - val_loss: 8.5800e-04\n",
            "Epoch 194/1000\n",
            "3912/3912 [==============================] - 1s 174us/step - loss: 6.6115e-04 - val_loss: 6.4860e-04\n",
            "Epoch 195/1000\n",
            "3912/3912 [==============================] - 1s 133us/step - loss: 5.8966e-04 - val_loss: 9.3595e-04\n",
            "Epoch 196/1000\n",
            "3912/3912 [==============================] - 1s 134us/step - loss: 5.7373e-04 - val_loss: 6.5369e-04\n",
            "Epoch 197/1000\n",
            "3912/3912 [==============================] - 1s 160us/step - loss: 6.3523e-04 - val_loss: 0.0010\n",
            "Epoch 198/1000\n",
            "3912/3912 [==============================] - 1s 174us/step - loss: 6.7948e-04 - val_loss: 8.8868e-04\n",
            "Epoch 199/1000\n",
            "3912/3912 [==============================] - 1s 152us/step - loss: 6.0722e-04 - val_loss: 8.7792e-04\n",
            "Epoch 200/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.8241e-04 - val_loss: 0.0011\n",
            "Epoch 201/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 7.5202e-04 - val_loss: 7.8365e-04\n",
            "Epoch 202/1000\n",
            "3912/3912 [==============================] - 0s 114us/step - loss: 6.3705e-04 - val_loss: 6.4018e-04\n",
            "Epoch 203/1000\n",
            "3912/3912 [==============================] - 0s 127us/step - loss: 6.3247e-04 - val_loss: 6.3899e-04\n",
            "Epoch 204/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.2776e-04 - val_loss: 0.0010\n",
            "Epoch 205/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.8337e-04 - val_loss: 7.2033e-04\n",
            "Epoch 206/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 6.1294e-04 - val_loss: 7.8657e-04\n",
            "Epoch 207/1000\n",
            "3912/3912 [==============================] - 1s 144us/step - loss: 6.8126e-04 - val_loss: 7.3051e-04\n",
            "Epoch 208/1000\n",
            "3912/3912 [==============================] - 1s 130us/step - loss: 6.2123e-04 - val_loss: 6.2719e-04\n",
            "Epoch 209/1000\n",
            "3912/3912 [==============================] - 1s 140us/step - loss: 6.2092e-04 - val_loss: 9.6476e-04\n",
            "Epoch 210/1000\n",
            "3912/3912 [==============================] - 1s 129us/step - loss: 6.8277e-04 - val_loss: 6.3400e-04\n",
            "Epoch 211/1000\n",
            "3912/3912 [==============================] - 1s 139us/step - loss: 6.2938e-04 - val_loss: 6.6678e-04\n",
            "Epoch 212/1000\n",
            "3912/3912 [==============================] - 0s 126us/step - loss: 6.3106e-04 - val_loss: 7.9976e-04\n",
            "Epoch 213/1000\n",
            "3912/3912 [==============================] - 1s 138us/step - loss: 6.6444e-04 - val_loss: 0.0011\n",
            "Epoch 214/1000\n",
            "3912/3912 [==============================] - 0s 124us/step - loss: 6.7745e-04 - val_loss: 8.8634e-04\n",
            "Epoch 215/1000\n",
            "3912/3912 [==============================] - 1s 132us/step - loss: 6.9530e-04 - val_loss: 6.5300e-04\n",
            "Epoch 216/1000\n",
            "3912/3912 [==============================] - 0s 122us/step - loss: 5.9127e-04 - val_loss: 6.8889e-04\n",
            "Best loss: 0.0006203985172162161\n",
            "Best loss id: 186\n",
            "This is the first entry in the file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noTMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhKe1IiKN1Xn",
        "colab_type": "code",
        "outputId": "ebe1efb7-05da-4331-bdf0-75aac3ed1a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "# Load model to memory\n",
        "run_id = 6\n",
        "part_id = 0\n",
        "model = load_model('models/nn_model_'+str(run_id)+'_'+str(part_id)+'.h5')\n",
        "model.load_weights('models/best_model_run_'+str(run_id)+'_'+str(part_id)+'.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "hidden_z (Dense)             (None, 256)               768       \n",
            "_________________________________________________________________\n",
            "hidden_z_0 (Dense)           (None, 6)                 1542      \n",
            "_________________________________________________________________\n",
            "hidden_z_1 (Dense)           (None, 32)                224       \n",
            "_________________________________________________________________\n",
            "output_z (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,567\n",
            "Trainable params: 2,567\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXbRWy9htRz",
        "colab_type": "code",
        "outputId": "78c95dd3-b684-4064-bbad-66bf0c7e4497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Get dataset\n",
        "(x_train_in_use, x_test_in_use,\n",
        "    y_train_in_use, y_test_in_use,\n",
        "    y_mean, y_std) = normalize_data(norm_input=True, norm_output=True)\n",
        "\n",
        "y_train_z = y_train_in_use[:,2]\n",
        "y_test_z = y_test_in_use[:,2]\n",
        "\n",
        "\n",
        "print(\"y mean:\", y_mean)\n",
        "print(\"y std:\", y_std)\n",
        "\n",
        "# x_train_in_use = x_train_in_use[:,[0,2,3,4,5]]\n",
        "# x_test_in_use = x_test_in_use[:,[0,2,3,4,5]]\n",
        "########################################\n",
        "\n",
        "# Analyze weights\n",
        "# first_weights = model.trainable_weights[0]\n",
        "# # print(first_weights.shape)\n",
        "# for input_id in range(6):\n",
        "#     w = K.eval(first_weights[input_id])\n",
        "#     print(\"Weights from input\", input_id)\n",
        "#     print(\"Mean:\", np.mean(w))\n",
        "#     print(\"Std:\", np.std(w))\n",
        "#     print()\n",
        "\n",
        "# Analyze activations\n",
        "# layer_outputs = [layer.output for layer in model.layers[1:]]\n",
        "# Extracts the outputs of the top 12 layers\n",
        "# activation_model = Model(inputs=model.input,\n",
        "#     outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
        "# activations = activation_model.predict(x_test_in_use) \n",
        "# first_layer_activation = activations[0]\n",
        "# print(first_layer_activation[0].shape)\n",
        "# plt.matshow(first_layer_activation)\n",
        "\n",
        "# results = model.evaluate(x_test_in_use, [y_test_x, y_test_y, y_test_z], batch_size=128)\n",
        "results = model.evaluate(x_test_in_use, y_test_z, batch_size=32)\n",
        "print('test loss:', results)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y mean: [-0.15933413  0.03610403  5.56289522]\n",
            "y std: [1.37827808 2.27332637 1.74280007]\n",
            "413/413 [==============================] - 0s 75us/step\n",
            "test loss: 0.0006734766650969753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3JwVQBaHfR",
        "colab_type": "code",
        "outputId": "eca33509-4f44-41c3-c606-4fb363c3b974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Run this to test the model on test_dataset\n",
        "def test_model(model):\n",
        "    # run_id = 3\n",
        "    # model = load_model('nn_model_'+str(run_id)+'.h5')\n",
        "    # model.load_weights('best_model_run_'+str(run_id)+'.h5')\n",
        "\n",
        "    classes = model.predict(x_test_in_use)\n",
        "\n",
        "\n",
        "    # ids = [136, 74, 9, 12, 49, 1, 16, 3, 11, 5, 4, 2, 15, 7]\n",
        "    ids = [103, 30, 17, 6, 23, 5, 20, 2, 3, 24, 18, 7, 1, 8]\n",
        "\n",
        "    range_min = np.arange(1.0, 8.0, 0.5)\n",
        "    range_max = np.arange(1.5, 8.5, 0.5)\n",
        "\n",
        "    # print(range_min)\n",
        "    # print(range_max)\n",
        "\n",
        "    # gt_plot = y_test[ids][:,2]\n",
        "    # est_plot = classes[2][ids][:,0]\n",
        "\n",
        "    y_mean = [-0.15933413, 0.03610403, 5.56289522]\n",
        "    y_std = [1.37827808, 2.27332637, 1.74280007]\n",
        "\n",
        "    gt_plot = y_test_in_use[ids][:,2]*y_std[2] + y_mean[2]\n",
        "    est_plot = classes[ids][:,0]*y_std[2] + y_mean[2]\n",
        "    # print(gt_plot)\n",
        "    # print(est_plot)\n",
        "\n",
        "    plt.plot(gt_plot, gt_plot)\n",
        "    plt.plot(gt_plot, est_plot)\n",
        "    plt.title('Height estimate')\n",
        "    plt.ylabel('Height')\n",
        "    plt.xlabel('Height')\n",
        "    plt.legend(['gt', 'estimate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def find_ids_in_even_range(model):\n",
        "    classes = model.predict(x_test_in_use)\n",
        "\n",
        "    range_min = np.arange(1.0, 8.0, 0.5)\n",
        "    range_max = np.arange(1.5, 8.5, 0.5)\n",
        "\n",
        "    output_size = 1\n",
        "    new_id = []\n",
        "    count = 0\n",
        "    i = 0\n",
        "    print(min(y_test[:,2]))\n",
        "    while count < len(range_min):\n",
        "    # for i in ids:\n",
        "        if y_test[i][2] > range_min[count] and y_test[i][2] < range_max[count]:\n",
        "            # print(\"ID:\", i)\n",
        "            # print(\"Ground truth:\", end=\" [\")\n",
        "            # for j in range(output_size):\n",
        "            #     print(str(round(y_test_in_use[i][j], 4)).rjust(8), end=\"\")\n",
        "            # print(']')\n",
        "            # print(\"Prediction:\", end=\"   [\")\n",
        "            # for j in range(output_size):\n",
        "            #     print(str(round(classes[j][i][0]*y_std[j] + y_mean[j], 4)).rjust(8), end=\"\")\n",
        "            #     print(str(round(classes[j][i][0], 4)).rjust(8), end=\"\")\n",
        "            #     print(str(round(classes[i][0], 4)).rjust(8), end=\"\")\n",
        "            # print(']\\n')\n",
        "            count += 1\n",
        "            new_id.append(i)\n",
        "            i = 0\n",
        "        i += 1\n",
        "\n",
        "    print(new_id)\n",
        "\n",
        "\n",
        "# find_ids_in_even_range(model)\n",
        "test_model(model)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngjfGmABlBW+MMQHq/wH/Bqfy2xMfcgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVvJhwR2Nrm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# 4.\n",
        "# 0.0912 - Veldig bra\n",
        "# batch_size=2\n",
        "# model.add(Dense(units=128, activation='relu', input_shape=(6,),\n",
        "#                   kernel_initializer=random_uniform_initializer,\n",
        "#                   activity_regularizer=regularizers.l1(0.001)))\n",
        "# Dropout 0.1\n",
        "\n",
        "\n",
        "####################\n",
        "# 5. 0.2369 stabilt\n",
        "\n",
        "####################\n",
        "# 6. 0.0871 bra!\n",
        "# random_uniform_initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "\n",
        "# model.add(Dense(units=128, activation='relu', input_shape=(6,),\n",
        "#                 kernel_initializer=random_uniform_initializer,\n",
        "#                 # kernel_regularizer=regularizers.l2(0.01),\n",
        "#                 activity_regularizer=regularizers.l1(0.0001)))\n",
        "# model.add(Dropout(0.1))\n",
        "# batch_size=1\n",
        "# adam = Adam(lr=0.0001, beta_1=0.8, beta_2=0.9)\n",
        "\n",
        "####################\n",
        "# 7. 0.0742 Nice!\n",
        "# random_uniform_initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "\n",
        "# model.add(Dense(units=128, activation='relu', input_shape=(6,),\n",
        "#                 kernel_initializer=random_uniform_initializer,\n",
        "#                 # kernel_regularizer=regularizers.l2(0.01),\n",
        "#                 activity_regularizer=regularizers.l1(0.0001)))\n",
        "# model.add(Dropout(0.1))\n",
        "# batch_size=1\n",
        "# adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PITGS1xn3r23",
        "colab_type": "code",
        "outputId": "b7310026-603d-43b1-bdf2-7817687226fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "Backup of the model code box from 15.02.2020\n",
        "\n",
        "\n",
        "# Hyperparameters and architecture\n",
        "run_id = 2\n",
        "\n",
        "init_weight_limit = 0.05\n",
        "hidden_units = [24]\n",
        "activation_fn = 'relu'\n",
        "dropout_rate = 0.3\n",
        "batch_size = 64\n",
        "max_epochs = 100\n",
        "learning_rate = 0.001\n",
        "use_optimizer = 'adam' # {'adam', 'sgd'}\n",
        "use_loss = 'mean_squared_error' # {'mean_squared_error' , 'mean_squared_logarithmic_error', 'mean_absolute_error'}\n",
        "\n",
        "####################################\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "random_uniform_initializer = RandomUniform(minval=-init_weight_limit, maxval=init_weight_limit, seed=None)\n",
        "\n",
        "# Create model\n",
        "# model.add(Dense(units=hidden_units[0], activation=activation_fn, input_shape=(6,),\n",
        "#                 kernel_initializer=random_uniform_initializer))\n",
        "# model.add(Dropout(dropout_rate))\n",
        "\n",
        "inputs = Input(shape=(6,), name=\"input\")\n",
        "\n",
        "layer_x = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "                kernel_initializer=random_uniform_initializer, name=\"hidden_x\")(inputs)\n",
        "layer_y = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "                kernel_initializer=random_uniform_initializer, name=\"hidden_y\")(inputs)\n",
        "layer_z = Dense(units=hidden_units[0], activation=activation_fn,\n",
        "                kernel_initializer=random_uniform_initializer, name=\"hidden_z\")(inputs)\n",
        "\n",
        "output_x = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_x\")(layer_x)\n",
        "output_y = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_y\")(layer_y)\n",
        "output_z = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_z\")(layer_z)\n",
        "\n",
        "\n",
        "model = Model(inputs=inputs, outputs=[output_x, output_y, output_z])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "# Add more layers if hidden_layers > 1\n",
        "# for i in range(len(hidden_units)-1):\n",
        "#     model.add(Dense(units=hidden_units[i+1], activation=activation_fn, input_shape=(6,),\n",
        "#                 kernel_initializer=random_uniform_initializer))\n",
        "#     model.add(Dropout(dropout_rate))\n",
        "\n",
        "# Output layer\n",
        "# model.add(Dense(units=1, kernel_initializer=random_uniform_initializer))\n",
        "\n",
        "# Choose optimizer\n",
        "if use_optimizer == 'adam':\n",
        "    # Original : adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "elif use_optimizer == 'sgd':\n",
        "    # Original: sgd = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
        "    opt = SGD(lr=0.0001)\n",
        "else:\n",
        "    opt = None\n",
        "\n",
        "# model.compile(loss=losses.mean_squared_error, optimizer=opt, metrics=['accuracy'])\n",
        "# model.compile(loss=use_loss, optimizer=opt)\n",
        "\n",
        "# Multitasking:\n",
        "# model.compile(loss={'output_x': use_loss, 'output_y': use_loss, 'output_z': use_loss}, optimizer=opt)\n",
        "model.compile(loss=use_loss, optimizer=opt)\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "# model.fit({'input':x_train_std, 'output_x':y_train_x, 'output_y':y_train_y, 'output_z':y_train_z}, batch_size=batch_size, nb_epoch=nb_epoch)\n",
        "\n",
        "\n",
        "# Run this to train the model\n",
        "STAMP = 'best_model_run_'+str(run_id)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "bst_model_path = STAMP + '.h5'\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# history = model.fit(x_train_std, y_train_std, validation_split=0.2, epochs=max_epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Multitasking:\n",
        "# history = model.fit({'input':x_train_std, 'output_x':y_train_x, 'output_y':y_train_y, 'output_z':y_train_z}, validation_split=0.2, epochs=max_epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "history = model.fit(x_train_std, [y_train_x, y_train_y, y_train_z],\n",
        "                    validation_split=0.2,\n",
        "                    # validation_data=({'input':x_test_std}, {'output_x':y_test_x, 'output_y':y_test_y, 'output_z':y_test_z}),\n",
        "                    epochs=max_epochs, batch_size=batch_size, verbose=1,\n",
        "                    callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "\n",
        "model.save('nn_model_'+str(run_id)+'.h5')\n",
        "\n",
        "val_loss_history = history.history['val_loss']\n",
        "min_val_loss = min(val_loss_history)\n",
        "min_val_loss_id = val_loss_history.index(min_val_loss) + 1\n",
        "# print('min_val_loss:', min_val_loss)\n",
        "# print('min_val_loss_id:', min_val_loss_id)\n",
        "\n",
        "make_report(run_id, min_val_loss, min_val_loss_id, init_weight_limit,\n",
        "    hidden_units, activation_fn, dropout_rate, batch_size,\n",
        "    max_epochs, learning_rate, use_optimizer, use_loss)\n",
        "\n",
        "show_and_save_multi_training_history(history, run_id)\n",
        "show_and_save_training_history(history, run_id)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBackup of the model code box from 15.02.2020\\n\\n\\n# Hyperparameters and architecture\\nrun_id = 2\\n\\ninit_weight_limit = 0.05\\nhidden_units = [24]\\nactivation_fn = \\'relu\\'\\ndropout_rate = 0.3\\nbatch_size = 64\\nmax_epochs = 100\\nlearning_rate = 0.001\\nuse_optimizer = \\'adam\\' # {\\'adam\\', \\'sgd\\'}\\nuse_loss = \\'mean_squared_error\\' # {\\'mean_squared_error\\' , \\'mean_squared_logarithmic_error\\', \\'mean_absolute_error\\'}\\n\\n####################################\\n\\n# model = Sequential()\\n\\nrandom_uniform_initializer = RandomUniform(minval=-init_weight_limit, maxval=init_weight_limit, seed=None)\\n\\n# Create model\\n# model.add(Dense(units=hidden_units[0], activation=activation_fn, input_shape=(6,),\\n#                 kernel_initializer=random_uniform_initializer))\\n# model.add(Dropout(dropout_rate))\\n\\ninputs = Input(shape=(6,), name=\"input\")\\n\\nlayer_x = Dense(units=hidden_units[0], activation=activation_fn,\\n                kernel_initializer=random_uniform_initializer, name=\"hidden_x\")(inputs)\\nlayer_y = Dense(units=hidden_units[0], activation=activation_fn,\\n                kernel_initializer=random_uniform_initializer, name=\"hidden_y\")(inputs)\\nlayer_z = Dense(units=hidden_units[0], activation=activation_fn,\\n                kernel_initializer=random_uniform_initializer, name=\"hidden_z\")(inputs)\\n\\noutput_x = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_x\")(layer_x)\\noutput_y = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_y\")(layer_y)\\noutput_z = Dense(units=1, kernel_initializer=random_uniform_initializer, name=\"output_z\")(layer_z)\\n\\n\\nmodel = Model(inputs=inputs, outputs=[output_x, output_y, output_z])\\n# model.summary()\\n\\n\\n# Add more layers if hidden_layers > 1\\n# for i in range(len(hidden_units)-1):\\n#     model.add(Dense(units=hidden_units[i+1], activation=activation_fn, input_shape=(6,),\\n#                 kernel_initializer=random_uniform_initializer))\\n#     model.add(Dropout(dropout_rate))\\n\\n# Output layer\\n# model.add(Dense(units=1, kernel_initializer=random_uniform_initializer))\\n\\n# Choose optimizer\\nif use_optimizer == \\'adam\\':\\n    # Original : adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\\n    opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\\n\\nelif use_optimizer == \\'sgd\\':\\n    # Original: sgd = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\\n    opt = SGD(lr=0.0001)\\nelse:\\n    opt = None\\n\\n# model.compile(loss=losses.mean_squared_error, optimizer=opt, metrics=[\\'accuracy\\'])\\n# model.compile(loss=use_loss, optimizer=opt)\\n\\n# Multitasking:\\n# model.compile(loss={\\'output_x\\': use_loss, \\'output_y\\': use_loss, \\'output_z\\': use_loss}, optimizer=opt)\\nmodel.compile(loss=use_loss, optimizer=opt)\\n# model.summary()\\n\\n\\n# model.fit({\\'input\\':x_train_std, \\'output_x\\':y_train_x, \\'output_y\\':y_train_y, \\'output_z\\':y_train_z}, batch_size=batch_size, nb_epoch=nb_epoch)\\n\\n\\n# Run this to train the model\\nSTAMP = \\'best_model_run_\\'+str(run_id)\\nearly_stopping = EarlyStopping(monitor=\\'val_loss\\', patience=10)\\nbst_model_path = STAMP + \\'.h5\\'\\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\\n\\n# history = model.fit(x_train_std, y_train_std, validation_split=0.2, epochs=max_epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping, model_checkpoint])\\n\\n# Multitasking:\\n# history = model.fit({\\'input\\':x_train_std, \\'output_x\\':y_train_x, \\'output_y\\':y_train_y, \\'output_z\\':y_train_z}, validation_split=0.2, epochs=max_epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping, model_checkpoint])\\n\\nhistory = model.fit(x_train_std, [y_train_x, y_train_y, y_train_z],\\n                    validation_split=0.2,\\n                    # validation_data=({\\'input\\':x_test_std}, {\\'output_x\\':y_test_x, \\'output_y\\':y_test_y, \\'output_z\\':y_test_z}),\\n                    epochs=max_epochs, batch_size=batch_size, verbose=1,\\n                    callbacks=[early_stopping, model_checkpoint])\\n\\n\\nmodel.save(\\'nn_model_\\'+str(run_id)+\\'.h5\\')\\n\\nval_loss_history = history.history[\\'val_loss\\']\\nmin_val_loss = min(val_loss_history)\\nmin_val_loss_id = val_loss_history.index(min_val_loss) + 1\\n# print(\\'min_val_loss:\\', min_val_loss)\\n# print(\\'min_val_loss_id:\\', min_val_loss_id)\\n\\nmake_report(run_id, min_val_loss, min_val_loss_id, init_weight_limit,\\n    hidden_units, activation_fn, dropout_rate, batch_size,\\n    max_epochs, learning_rate, use_optimizer, use_loss)\\n\\nshow_and_save_multi_training_history(history, run_id)\\nshow_and_save_training_history(history, run_id)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPDQrV897MvE",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "* Maybe remove some of the input features.\n",
        "    * Find out which features contributes most and least to the prediction of each value (x,y,z) and remove those that does not contribute\n",
        "    * Try an \"autoencoder\" (i.e. one layer with fewer units than input and output units)\n",
        "    * Try three hidden layers, e.g. (128,4,128)"
      ]
    }
  ]
}