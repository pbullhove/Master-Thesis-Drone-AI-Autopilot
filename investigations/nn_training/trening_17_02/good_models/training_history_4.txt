
###############################################
Part: 0
Lowest value loss: 1.0145641712042002
Epoch with lowest value loss: 1

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1
Batch size: 32
Learning rate: 0.001
L2 regularization: 0.1
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
1.0145641712042002
###############################################
Part: 0
Lowest value loss: 1.0086283859228478
Epoch with lowest value loss: 8

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 0.1
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
1.0086283859228478
###############################################
Part: 0
Lowest value loss: 0.9622796051930158
Epoch with lowest value loss: 27

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 0.001
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.9622796051930158
###############################################
Part: 0
Lowest value loss: 0.46273087898890175
Epoch with lowest value loss: 75

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 0.0
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.46273087898890175
###############################################
Part: 0
Lowest value loss: 0.3717594702427204
Epoch with lowest value loss: 107

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.3717594702427204
###############################################
Part: 0
Lowest value loss: 0.25796249903165375
Epoch with lowest value loss: 180

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.25796249903165375
###############################################
Part: 0
Lowest value loss: 0.0006118397447010654
Epoch with lowest value loss: 157

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 3
Hidden units: [256, 6, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.0006118397447010654