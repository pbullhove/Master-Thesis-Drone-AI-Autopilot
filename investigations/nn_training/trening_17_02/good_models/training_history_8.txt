
###############################################
Part: 0
Lowest value loss: 0.111848176149718
Epoch with lowest value loss: 184

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 1
Hidden units: [32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.111848176149718
###############################################
Part: 1
Lowest value loss: 0.06730200307666342
Epoch with lowest value loss: 272

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 2
Hidden units: [256, 32]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.06730200307666342
###############################################
Part: 2
Lowest value loss: 0.06719774542427769
Epoch with lowest value loss: 346

# Dataset:
Normalize input: True
Normalize output: True

# Architecture:
Hidden layers: 2
Hidden units: [128, 16]
Activation function: relu
Optimizer: adam
Loss function: mean_squared_error

# Hyper parameters:
Max epochs: 1000
Batch size: 32
Learning rate: 0.001
L2 regularization: 1e-07
Dropout rate: 0.2
Initial weight limit: 0.1
###############################################
Best:
0.06719774542427769